# 전재우 데이터분석 포트폴리오
## 사용 가능 언어 및 라이브러리

> ![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white) ![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white) ![R](https://img.shields.io/badge/R-276DC3?style=for-the-badge&logo=r&logoColor=white) ![SAS SQL](https://img.shields.io/badge/SAS%20SQL-105481?style=for-the-badge&logo=sas&logoColor=white)

<br />


## 관심 분야 및 스킬
* 고차원 데이터 분석을 위한 변수 선택 및 차원 축소.  
* PC(Principle component), Factor(요인) 기반 기저변수 탐색.  
* 개인별 특성에 기반한 점수화, 분류모델 및 해석.
* 시계열 데이터 전처리 및 분석.  

<br />


# 프로젝트  
> 팀단위로 진행한 외부 프로젝트, 공모전을 정리한 내용입니다.
> 
> 자세한 내용 및 시각화 자료는 해당 폴더에 저장되어 있습니다.
> 
> 프로젝트는 코드를 제공할 수 없어 정리본만 작성했습니다.  

<br />

## 1. [공모전] 지역난방 열수요와 날씨 빅데이터를 융합한 열수요 예측

> **관련 URL :** [공모전 페이지](https://bd.kma.go.kr/contest/)
> 
> **핵심 역할** : 팀장, 데이터 보간 방식 및 주요 분석 개념(삼각변환, DTW) 제안, 지사별 분석 코드 작성
<details>
<summary>열기 / 닫기</summary>

### 사용 언어
![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white) ![R](https://img.shields.io/badge/R-276DC3?style=for-the-badge&logo=r&logoColor=white)


### 분석 개요

* 최근 공동주택의 공급과 거주 비중이 증가하면서, 이에 따라 지역난방 수요 역시 지속적으로 확대되고 있다.
* 공동주택의 열 수요는 외부 기상 요인의 영향을 크게 받기 때문에, 기온, 습도, 풍속 등 다양한 기상 변수에 기반한 정밀한 열수요 예측이 필요하다.
* 기존의 열수요 기반 데이터 뿐만 아니라 날씨 빅데이터를 융합한 열수요 예측 모델을 통해 더 정확한 예측을 목표로 한다.
<br />

### 분석 대상

* 학습 데이터: 2021.01.01 부터 2023.12.31 까지의 시계열 데이터가 시간 단위로 관측되었다. (총 499,301개)
* 평가 데이터: 2024년 1년의 데이터중 heat_demand를 제외한 나머지 변수가 주어졌을 때 지사별 heat_demand를 예측했다.  
* 총 19개의 지사가 데이터에 포함되어 있고, 대량의 연속결측이 발생한 변수가 있기 때문에 데이터 전처리가 핵심 과제였다.
* 평가 기준: RMSE
  
  | 변수명 | 설명 |
  | :--- | :--- |
  | tm | 날짜 및 시각 |
  | m_hr1 | 1시간 강수량 |
  | branch_ID | 지사명 |
  | hm | 정시 상대 습도 |
  | ta | 정시기온 |
  | si | ASOS 일사량 |
  | wd | 정시 10분 평균 풍향 |
  | ta_chi | 체감온도 |
  | ws | 정시 10분 평균 풍속 |
  | heat_demand | 시간당 지사별 열 공급량 |
  | m_day | 해당 시간까지의 일 강수량 |
  
<br />

### 분석 방안

##### 결측치 보간
* 기온(ta, 12 997개), 풍향(wd, 18 815개), 풍속(ws, 18 815개), 일강수량(rn_day, 18 626개), 시간 강수량(rn_hr1, 19 154개), 상대습도(hm, 39 717개), 일사량(si, 232 922개), 체감온도(ta_chi, 20개), 열수요(heat_demand, 23개) 변수에 결측치가 확인되었다.

* 연속으로 1개, 2개 결측된 관측값들에 대해서는 시계열 데이터에서 자주 사용되는 선형 보간을 진행했다.

* 3개 이상 연속으로 결측된 경우, 원본 데이터의 지역적 특성과 기상적 특성을 반영하기 위해 직전값과 클러스터 내 평균을 가중평균해 사용했다.

* 시계열 데이터에서 정확한 클러스터링을 위해 DTW(Dynamic Time Warping)을 기반으로한 계층적 클러스터링을 수행했다.

##### 도메인과 데이터를 고려한 파생변수 생성
* 시간에 대한 효과를 명확히 표한히가 위해 삼각함수 변환을 통한 주기성을 표현했다.

* 지연 변수를 통해 lag 효과를 표현해 모델이 학습할 수 있도록 했다.

##### 지사별 시계열 분해 및 모델학습
* 지사별 환경 차이점을 고려해, 지사 각각의 데이터를 통해 개별 모델을 학습했다.

* 시계열 데이터의 큰 추세와 세부적인 변화를 분석하고 변수의 중요도를 해석하기 위해 HP filter를 이용한 추세-잔차 이중모형을 설정했다.

* 각 추세-잔차에 대해 validation에서 가장 좋은 성능을 보인 XGboost + XGboost 조합을 최최종 모델로 선정했다. 

<br />

### 분석 결과 및 활용

* 최종 도출한 모델을 통해 기존 시행되는 사업중 하나인 "에코 마일리지" 사업의 고도화를 제안했다.

* 기존 방식은 전년도 열 사용량을 통해 해당 연도의 사용량 목표를 제시했다. 하지만 이러한 방식은 해당 연도의 기상상황을 전혀 반영하지 못하는 문제가 있다.

* 추세-잔차 모델을 통해 알아낸 세부적인 추세와 잔차의 변화에 영향을 주는 변수를 반영하여 더욱 세부적인 개인 절약 목표를 제시하여 더 합리적인 목표를 제공했다.

* 최종 제출 결과: 최종 RMSE 15.53으로 본선에 진출하여 입상했다.

<br />

### 개선점

* 모델에서 도출한 변수 중요도를 통해 더 세부적인 해석 결과를 제공해야한다. 이는 모델에서 분석이 끝나는 것이 아닌 더 세부적인 해석을 통해 실질적인 활용의 영역으로 나아가야 한다. 

* 지사별 데이터를 통해 학습했지만 데이터를 세분화 하면서 모델별 학습 데이터량이 줄어드는 문제가 발생했다. 지사 개별, 클러스터별, 전체 데이터 중 어느 데이터 단위가 더 합당할지 추가적인 기준이 있으면 분석 흐름이 개선될 것이다.

* 분석 당시의 팀원 개인의 편의성을 위해 개발 언어를 통일하지 않고 진행했다. 데이터 전처리 후 파일을 반출하고 후반 분석을 진행했기 때문에 문제가 생기진 않았지만, 추후 개발 및 문서화에 불리점이 있어 이를 개선하는 것이 좋을 것으로 판단된다.

* HP filter 기반 분해에서 각 시간 주기(월,일,시) 단위로 다른 파라메터를 통해 분해가 가능하다. 분석에서는 가장 세부 단위 한가지만을 사용했지만, 다양한 단위를 통해 분해 결과를 다각화 했다면 더 정확하고 해석에 이점이 있는 모델을 작성할 수 있을 것으로 예상된다.

</details>


---

## 2. [프로젝트] 의료 빅데이터 분석을 위한 질병 단계 기반 통합적 데이터과학 방법론 개발

> **관련 URL :** [NTIS 과제 정보](https://www.ntis.go.kr/project/pjtInfo.do?pjtId=2710018202&pageCode=TH_PJT_PJT_DTL)
>
> **핵심 역할 :** 석사연구원, 유전체 데이터 처리 및 질병 유형/원인 분석 모델 구현

<details>
<summary>열기 / 닫기</summary>


### 사용 언어 및 라이브러리
![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white) ![R](https://img.shields.io/badge/R-276DC3?style=for-the-badge&logo=r&logoColor=white)

### 분석 개요

* 의료 빅데이터를 활용하여 질병의 **예측, 원인 규명, 진단, 치료, 효과 검증**에 이르는 전 과정을 통합적으로 분석하는 것이 목표로 한다.
* 현재 프로젝트는 진행중으로, 질병의 근본적인 원인을 탐색하는 **질병 유형/원인 규명 단계**의 기초 모델링 연구에 집중하고 있다.
* 궁극적으로는 질병의 효과적인 예방과 치료에 기여하여 의학 및 데이터과학 분야의 학문적 발전에 기여하는 것을 목표로 한다.
<br />

### 분석 대상

* **주요 분석 데이터:** 질병의 근본적인 유전적 요인을 탐색하기 위한 유전체 데이터를 핵심적으로 다루고 있다.
* **데이터 특징:** 다수의 유전자 정보를 포함하는 고차원 데이터이며, 분석 결과의 신뢰도에 영향을 미치는 교란 변수가 존재할 가능성이 높다.
<br />

### 분석 방안

##### 질병 발생/원인 규명을 위한 기초 모델링
* **유전자 네트워크 분석:** 다수의 유전자 네트워크 데이터를 통합하고, 질병과 관련된 핵심적인 유전자 상호작용을 식별하기 위해 **잠재 공간 모형(Latent Space Model)**을 구현하고 있다.
* **인과관계 추론:** 유전체 데이터 분석 시 결과의 신뢰도를 저해하는 교란 변수를 효과적으로 제어하기 위해, **베이지안 비모수 방법론**에 기반한 변수 선택 기법을 개발 및 적용하고 있다.
<br />

### 중간 결과 및 기대효과

* **개인 기여:** 고차원의 유전체 데이터에 대한 변수 선택 파이프라인을 구축했으며, 질병 발생과 관련된 유전자 분석을 위한 기초 모형을 성공적으로 구현했다.
* **기대 효과:** 본 1차년도 연구를 통해 질병의 근본적인 유전적 원인을 규명하는 모델의 기반을 마련하고, 인과추론의 정확성을 높여 분석 결과의 신뢰도를 확보할 것으로 기대한다.
<br />

### 향후 연구 계획

* **모델 고도화:** 현재 구현된 기초 모델을 바탕으로, 더 복잡한 유전자 상호작용을 포착할 수 있도록 모델 구조를 발전시킨다.
* **2차년도 연구 연계:** 1차년도에 개발된 유전자 분석 모델의 결과를 바탕으로, 2차년도에는 질병의 진단 및 치료 단계 모델과의 연계 방안을 개발한다.

</details>

---


## 3. [DACON] Boost up AI 2025 : 신약 개발 경진대회

> **관련 URL :** [데이콘 페이지](https://dacon.io/competitions/official/236518/overview/description)
> 
> **핵심 역할** : 주요 데이터 해석 및 모델 학습
<details>
<summary>열기 / 닫기</summary>

### 사용 언어
![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white) ![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)


### 분석 개요

* 인체 내 약물 대사의 핵심 효소인 CYP3A4는 전체 Cytochrome P450 대사의 절반 이상을 차지하며, 대부분의 약물 분해에 관여한다.
* CYP3A4의 활성을 조절하는 약물을 함께 복용하면, 혈중 약물 농도가 불안정해져 약효가 감소하거나 부작용 위험이 커질 수 있다.
* 본 연구는 이러한 문제를 해결하기 위해, 약물의 분자 구조만으로 CYP3A4 저해율을 예측하는 AI 모델을 개발하는 것을 목표로 한다.
<br />

### 분석 대상

* 학습 데이터: 1,681개의 분자에 대한 고유 구조 문자열(`Canonical SMILES`)과 억제정도(`Inhibition`)가 관측됐다.
* 평가 데이터: 100개의 분자에 대한 고유 구조 문자열을 통해 억제정도를 예측했다.  
* 평가 기준: NRMSE와 Pearson 상관 계수를 결합한 지표
  
|`ID` |`Canonical_Smiles` |`Inhibition`|
  |:------|:------|------:|
  |TRAIN_0000	|`Cl.OC1(Cc2cccc(Br)c2)CCNCC1`|12.5|
  |TRAIN_0001	|`Brc1ccc2OCCc3ccnc1c23`|4.45|
  |TRAIN_0002	|`CC1(CO)CC(=NO1)c2cc(c(F)cc2Cl)[N+](=O)[O-]`|4.92|
  
<br />

### 분석 방안

##### 데이터적 특성
* 분자 구조를 나타내는 Canonical_Smiles 문자열로부터 AI 모델이 학습할 수 있는 수치적 특징을 생성하기 위해, rdkit 라이브러리를 사용했다. 이를 통해 약 170종의 물리화학적 기술자와 11가지 종류의 분자 지문(fingerprint)을 계산하여 데이터셋을 생성했다.

* 학습 데이터의 양이 많지 않아 직접 DL 모델을 작성하는 것은 유의하지 않을 것이라고 판단했다. 기본 모델로 고차원 상황을 해결할 수 있는 tree based 모델인 LigthGBM을 사용해 생성한 분자 지문들의 성능을 평가했다.

##### 모델 작성과 앙상블
* LigthGBM을 통해 평가된 상위 5개의 분자 지문에 대해서 추가로 RandomForest를 작성했다.

* 레이어를 고정한 사전학습 GNN 모델인 GROVER를 사용하여 도출한 최종 embeding을 통해 Ridge 회귀모형을 작성했다.
  
  (*레이어 고정 이유: 해당 데이터를 통해 새롭게 학습한 모델 생성이 목적이 아니라 embeding을 통한 변수 추출이 목적)

* 더 robust한 결과를 위해 앙상블을 진행하였고, 가중치는 optuna를 통해 도출했다.

<br />

### 분석 결과 및 활용

* Public score 기준 160등, private score 기준 216등으로 대회를 마감했다.

* 예측에 초점이 맞춰진 대회이기 때문에 해석과 변수에 대한 분석은 진행하지 않았다.


<br />

### 개선점

* 주어진 데이터가 고유 구조 문자열과 억제정도뿐이었기에 분석을 위한 변수 생성에 어려움을 겪었다. 데이터에 대한 도메인 지식이 있는 전문가와 협업을 한다면 데이터 준비 단계가 더욱 원활할 것이다.

* 분자 구조또한 네트워크 데이터의 일종으로 파악해 GNN을 활용한 분석이 유의미할것으로 예상했다. 다만 새로운 모델을 전체적으로 학습하기엔 데이터의 수가 너무 적어 사전학습된 모델을 활용했지만, 그다지 좋은 성능을 내지 못했다. 데이터 양과 특성을 고려한 모델 선택에 더 신중하고 근거를 찾을 수 있어야한다.


</details>

---

## 4. [공모전] 클러스터링을 통한 지하수 유형화 및 신규 가뭄취약지역 시설 추천

> **관련 URL :** [공모전 페이지](https://www.gims.go.kr/competPage.do)
> 
> **핵심 역할** : 팀장, 데이터 수집 및 핵심 분석방법론 제안(군집화 및 유형예측).
<details>
<summary>열기 / 닫기</summary>

### 사용 언어
![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)


### 분석 개요

* 
* 
* 
<br />

### 분석 대상

* 활용 데이터: 
* 
* 
  
  | 데이터명 | 데이터 내용 | 출처 |
  
<br />

### 분석 방안

##### 데이터 전처리
* 상관분석 및 pc를 통한 변수 정리 및 선택

* 

* 


##### 클러스터링을 통한 유형화 및 해석
* 

* 

##### 신규 지역을 위한 유형 분류기 학습 및 적용
* 

* 

* 

<br />

### 분석 결과 및 활용

* 

* 

* 


<br />

### 개선점

* 

* 

* 


</details>

---
