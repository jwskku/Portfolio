{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# Import\n",
    "# ==================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from sklearn.metrics import root_mean_squared_error, make_scorer\n",
    "from sklearn.model_selection import KFold, cross_validate, GridSearchCV\n",
    "from sklearn.base import clone\n",
    "import optuna\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from argparse import Namespace\n",
    "from sklearn.linear_model import Ridge\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Feature Extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# Feature Extractor: Baseline Descriptors\n",
    "# ==================================================\n",
    "\n",
    "def DescriptorFarm(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    from rdkit import Chem\n",
    "    from rdkit.Chem import AllChem, Descriptors\n",
    "    from rdkit.Chem.Descriptors3D import descList as _desc3d_list\n",
    "    from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "\n",
    "    # 2D Descriptors\n",
    "    desc2d = [(n, f) for n, f in Descriptors._descList if not n.startswith('fr_')]\n",
    "    names2d = {n for n, _ in desc2d}\n",
    "\n",
    "    # 3D Descriptors\n",
    "    desc3d = [(n, f) for n, f in _desc3d_list if not n.startswith('fr_') and n not in names2d]\n",
    "\n",
    "    # Significant SMARTS Patterns\n",
    "    smarts_defs = {\n",
    "        'Imidazole': ['c1ncnc1'],\n",
    "        'Pyrazole': ['n1nccc1'],\n",
    "        'Thiazole': ['c1cscn1'],\n",
    "        'Triazole': ['n1nncc1', 'c1ncnn1'],\n",
    "        'Toluene': ['Cc1ccccc1'],\n",
    "        'N-Ethyllformamide': ['O=CNCC[aR]'],\n",
    "        'Amino_Arylmethane': ['[aR]CN'],\n",
    "        'N-Phenethylformamide': ['O=CNCCc1ccccc1'],\n",
    "        'Carbamate': ['OC(=O)N'],\n",
    "        'Benzodioxole': ['c1cc2OCOc2cc1'],\n",
    "        'Furan': ['c1ccoc1'],\n",
    "        'Terminal_Alkyne': ['[C]#[CH]'],\n",
    "        'Primary_Amine': ['[CX4][NH2]'],\n",
    "        'Sec_Cyclic_Amine': ['[CX4;R][NH1;R][CX4;R]'],\n",
    "        'Cyclopropyl_Amine': [\n",
    "        'C(C)(C)(C)N(C1CC1)C(C)(C)(C)',\n",
    "           'C(=C)(C)N(C1CC1)C(=C)(C)',\n",
    "           'C(#C)N(C1CC1)C(#C)',\n",
    "           '[H]N(C1CC1)[H]'\n",
    "        ],\n",
    "        'Hydroquinone':[\n",
    "            'Oc1ccc(O)cc1',\n",
    "            'Oc1ccccc1(O)',\n",
    "            'O=c1ccc(=O)cc1',\n",
    "            'O=c1ccccc1(=O)'\n",
    "        ],\n",
    "        'Epoxide': ['C1OC1'],\n",
    "        'Tertiary_Amine': ['N1([CX4])CCN(CC1)[CX4]'],\n",
    "        'Alkylphenol': [\n",
    "            '[OH]c1ccc(C([C,H])([C,H])[C,H])cc1',\n",
    "            '[OH]c1ccccc1(C([C,H])([C,H])[C,H])'\n",
    "        ],\n",
    "        'Alkylaromatic_Ether': [\n",
    "            'c1c([CX4])cc[c](O[CX4])c1',\n",
    "            'c1cc[c](O[CX4])c([CX4])c1'\n",
    "        ],\n",
    "        'Arenes':[\n",
    "            'c1ccc2c(c1)CCCO2',\n",
    "            '[C,H]N(C(c1ccccc1)c2ccccc2)[C,H]',\n",
    "            'c1ccccc1[CX4]C=C(C)(C)',\n",
    "            'c1ccccc1[CX4]C=C(=C)'\n",
    "        ],\n",
    "        'Alkoxybenzene': ['c1ccc(OC)cc1'],\n",
    "    }\n",
    "    smarts_patterns = {\n",
    "        name: [Chem.MolFromSmarts(s) for s in smarts_defs[name]]\n",
    "        for name in smarts_defs\n",
    "    }\n",
    "\n",
    "    # Row-Wise Computation\n",
    "    records = []\n",
    "    for smi in df['Canonical_Smiles']:\n",
    "        rec = {}\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        if mol is None:\n",
    "            for name, _ in desc2d + desc3d:\n",
    "                rec[name] = None\n",
    "            for col in [\n",
    "                'Murcko_num_rings',\n",
    "                'Murcko_max_ring_size',\n",
    "                'Murcko_min_ring_size',\n",
    "                'Murcko_mean_ring_size',\n",
    "                'Murcko_heavy_atom_count',\n",
    "                'Murcko_heteroatom_count',\n",
    "                'Murcko_mol_wt'\n",
    "            ]:\n",
    "                rec[col] = None\n",
    "            for name in smarts_patterns:\n",
    "                rec[name] = None\n",
    "        else:\n",
    "            # 2D Descriptors\n",
    "            for name, func in desc2d:\n",
    "                try:\n",
    "                    rec[name] = func(mol)\n",
    "                except:\n",
    "                    rec[name] = None\n",
    "\n",
    "            # 3D Descriptors\n",
    "            mol3d = Chem.AddHs(mol)\n",
    "            try:\n",
    "                AllChem.EmbedMolecule(mol3d, AllChem.ETKDG())\n",
    "                AllChem.MMFFOptimizeMolecule(mol3d)\n",
    "                for name, func in desc3d:\n",
    "                    try:\n",
    "                        rec[name] = func(mol3d)\n",
    "                    except:\n",
    "                        rec[name] = None\n",
    "            except:\n",
    "                for name, _ in desc3d:\n",
    "                    rec[name] = None\n",
    "\n",
    "            # Murcko Scaffold\n",
    "            try:\n",
    "                scaffold = MurckoScaffold.GetScaffoldForMol(mol)\n",
    "                ri = scaffold.GetRingInfo()\n",
    "                ring_sizes = [len(r) for r in ri.AtomRings()]\n",
    "                num_rings = len(ring_sizes)\n",
    "                rec['Murcko_num_rings'] = num_rings\n",
    "                rec['Murcko_max_ring_size'] = max(ring_sizes) if ring_sizes else 0\n",
    "                rec['Murcko_min_ring_size'] = min(ring_sizes) if ring_sizes else 0\n",
    "                rec['Murcko_mean_ring_size'] = sum(ring_sizes)/num_rings if num_rings else 0\n",
    "                rec['Murcko_heavy_atom_count'] = scaffold.GetNumHeavyAtoms()\n",
    "                heteros = sum(1 for atom in scaffold.GetAtoms()\n",
    "                             if atom.GetAtomicNum() not in (6,1))\n",
    "                rec['Murcko_heteroatom_count'] = heteros\n",
    "                rec['Murcko_mol_wt'] = Descriptors.MolWt(scaffold)\n",
    "            except Exception:\n",
    "                for col in ['Murcko_num_rings',\n",
    "                            'Murcko_max_ring_size',\n",
    "                            'Murcko_min_ring_size',\n",
    "                            'Murcko_mean_ring_size',\n",
    "                            'Murcko_heavy_atom_count',\n",
    "                            'Murcko_heteroatom_count',\n",
    "                            'Murcko_mol_wt']:\n",
    "                    rec[col] = None\n",
    "\n",
    "            # Matching SMARTS Patterns\n",
    "            for name, patterns in smarts_patterns.items():\n",
    "                try:\n",
    "                    rec[name] = any(mol.HasSubstructMatch(pat) for pat in patterns)\n",
    "                except:\n",
    "                    rec[name] = None\n",
    "\n",
    "        records.append(rec)\n",
    "\n",
    "    # Result\n",
    "    desc_df = pd.DataFrame(records, index = df.index)\n",
    "\n",
    "    return pd.concat([df, desc_df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# Feature Extractor: Fingerprints\n",
    "# ==================================================\n",
    "\n",
    "def FingerprintFarm(\n",
    "        df: pd.DataFrame,\n",
    "        smiles_col: str = 'Canonical_Smiles',\n",
    "        fp: str | None = None\n",
    ") -> pd.DataFrame:\n",
    "    import os\n",
    "    from rdkit import Chem, RDConfig\n",
    "    from rdkit.Chem import rdMolDescriptors, rdFingerprintGenerator\n",
    "    from rdkit.Chem.rdmolops import PatternFingerprint\n",
    "    from rdkit.Chem.Pharm2D.SigFactory import SigFactory\n",
    "    from rdkit.Chem.Pharm2D import Generate as Pharm2DGen\n",
    "    from rdkit.Chem.rdReducedGraphs import GetErGFingerprint\n",
    "    from rdkit.Chem.rdMHFPFingerprint import MHFPEncoder\n",
    "    from rdkit.Chem import ChemicalFeatures\n",
    "\n",
    "    # Fingerprint Generator\n",
    "    fp_name = None\n",
    "    fp_gen = None\n",
    "    if fp:\n",
    "        nm = fp.lower()\n",
    "        if nm == 'rdkit':\n",
    "            fp_name, fp_gen = 'rdkit', rdFingerprintGenerator.GetRDKitFPGenerator(fpSize = 2048)\n",
    "        elif nm == 'atompairs':\n",
    "            fp_name, fp_gen = 'atompairs', rdFingerprintGenerator.GetAtomPairGenerator(fpSize = 2048)\n",
    "        elif nm == 'topologicaltorsions':\n",
    "            fp_name, fp_gen = 'topologicaltorsions', rdFingerprintGenerator.GetTopologicalTorsionGenerator(fpSize = 2048)\n",
    "        elif nm == 'ecfp4':\n",
    "            fp_name, fp_gen = 'ecfp4', rdFingerprintGenerator.GetMorganGenerator(radius = 2, fpSize = 2048)\n",
    "        elif nm == 'ecfp6':\n",
    "            fp_name, fp_gen = 'ecfp6', rdFingerprintGenerator.GetMorganGenerator(radius = 3, fpSize = 2048)\n",
    "        elif nm == 'maccs':\n",
    "            fp_name = 'maccs'\n",
    "        elif nm == 'pattern':\n",
    "            fp_name = 'pattern'\n",
    "        elif nm == '2dpharmacophore':\n",
    "            fdef = os.path.join(RDConfig.RDDataDir, 'BaseFeatures.fdef')\n",
    "            featFactory = ChemicalFeatures.BuildFeatureFactory(fdef)\n",
    "            sigFactory = SigFactory(featFactory, minPointCount = 2, maxPointCount = 3, trianglePruneBins = False)\n",
    "            sigFactory.SetBins([(0, 2), (2, 5), (5, 8)])\n",
    "            sigFactory.Init()\n",
    "            fp_name, fp_gen = '2dpharmacophore', sigFactory\n",
    "        elif nm == 'erg':\n",
    "            fp_name = 'erg'\n",
    "        elif nm == 'mhfp':\n",
    "            fp_name, fp_gen = 'mhfp', MHFPEncoder()\n",
    "        elif nm == 'secfp':\n",
    "            fp_name, fp_gen = 'secfp', MHFPEncoder()\n",
    "\n",
    "    # Row-Wise Computation\n",
    "    records = []\n",
    "    for smi in df[smiles_col]:\n",
    "        rec = {}\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        if fp_name is None or mol is None:\n",
    "            if fp_name:\n",
    "                for i in range(2048):\n",
    "                    rec[f'{fp_name}_{i}'] = None\n",
    "        else:\n",
    "            if fp_name in ('rdkit', 'atompairs', 'topologicaltorsions', 'ecfp4', 'ecfp6'):\n",
    "                bv = fp_gen.GetFingerprint(mol)\n",
    "                bits = [int(b) for b in bv.ToBitString()]\n",
    "            elif fp_name == 'maccs':\n",
    "                bv = rdMolDescriptors.GetMACCSKeysFingerprint(mol)\n",
    "                bits = [int(b) for b in bv.ToBitString()]\n",
    "            elif fp_name == 'pattern':\n",
    "                bv = PatternFingerprint(mol)\n",
    "                bits = [int(b) for b in bv.ToBitString()]\n",
    "            elif fp_name == '2dpharmacophore':\n",
    "                bv = Pharm2DGen.Gen2DFingerprint(mol, fp_gen)\n",
    "                bits = [int(b) for b in bv.ToBitString()]\n",
    "            elif fp_name == 'erg':\n",
    "                try:\n",
    "                    bv = GetErGFingerprint(mol)\n",
    "                    bits = list(bv)\n",
    "                except KeyError:\n",
    "                    bits = [None] * 2048\n",
    "            elif fp_name == 'mhfp':\n",
    "                bv = fp_gen.EncodeMol(mol)\n",
    "                bits = list(bv)\n",
    "            elif fp_name == 'secfp':\n",
    "                bv = fp_gen.EncodeSECFPMol(mol)\n",
    "                bits = list(bv)\n",
    "            else:\n",
    "                bits = [0] * 2048\n",
    "            for i, bit in enumerate(bits):\n",
    "                rec[f'{fp_name}_{i}'] = bit\n",
    "        records.append(rec)\n",
    "\n",
    "    fp_df = pd.DataFrame(records, index = df.index)\n",
    "    return pd.concat([df, fp_df], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# CV Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# CV Settings\n",
    "# ==================================================\n",
    "\n",
    "# Evalutation Score\n",
    "def ScoreFarm_(y_true, y_pred):\n",
    "    nrmse = root_mean_squared_error(y_true, y_pred) / (np.max(y_true) - np.min(y_true))\n",
    "    pearson = np.clip(np.corrcoef(y_true, y_pred)[0, 1], 0, 1)\n",
    "    return 0.5 * (1 - np.minimum(nrmse, 1) + pearson)\n",
    "\n",
    "ScoreFarm = make_scorer(ScoreFarm_, greater_is_better = True)\n",
    "\n",
    "# 10-Fold CV\n",
    "cv = KFold(n_splits = 10, shuffle = True, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# LGBM: Preprocessing\n",
    "# ==================================================\n",
    "\n",
    "# Baseline Descriptors\n",
    "train = pd.read_csv('train.csv')\n",
    "train = DescriptorFarm(train)\n",
    "\n",
    "# Fingerprints List\n",
    "fp_list = [\n",
    "    'rdkit', 'atompairs', 'topologicaltorsions', 'ecfp4', 'ecfp6',\n",
    "    'maccs', 'pattern', '2dpharmacophore', 'erg', 'mhfp', 'secfp'\n",
    "]\n",
    "\n",
    "# Inputs Dictionary\n",
    "x = {'base': train.drop(['ID', 'Canonical_Smiles', 'Inhibition'], axis = 1)}\n",
    "for i in fp_list:\n",
    "    tmp = FingerprintFarm(train, fp = i)\n",
    "    tmp = tmp.drop(['ID', 'Canonical_Smiles', 'Inhibition'], axis = 1)\n",
    "    x[i] = tmp\n",
    "\n",
    "# Train Output\n",
    "y = train['Inhibition']\n",
    "\n",
    "# Constant Column Removal\n",
    "for i in x.keys():\n",
    "    nonconst_cols = x[i].columns[x[i].nunique() > 1]\n",
    "    x[i] = x[i][nonconst_cols]\n",
    "\n",
    "# ==================================================\n",
    "# LGBM: Model Definition\n",
    "# ==================================================\n",
    "\n",
    "# Base Estimator\n",
    "lgbm = LGBMRegressor(\n",
    "    objective = 'regression',\n",
    "    subsample_freq = 1,\n",
    "    random_state = 42,\n",
    "    device = 'gpu',\n",
    "    verbose = -1\n",
    ")\n",
    "\n",
    "# Models Dictionary\n",
    "md = {}\n",
    "for i in ['base'] + fp_list:\n",
    "    md[i] = clone(lgbm)\n",
    "\n",
    "# Dense Cases (n > p)\n",
    "dense_list = ['base', 'maccs', 'erg']\n",
    "\n",
    "# Sparse Cases (n < p)\n",
    "sparse_list = [\n",
    "    'rdkit', 'atompairs', 'topologicaltorsions', 'ecfp4',\n",
    "    'ecfp6', 'pattern', '2dpharmacophore', 'mhfp', 'secfp'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# LGBM: Model Selection for Dense Cases\n",
    "# ==================================================\n",
    "\n",
    "# Dense Cases (n > p)\n",
    "for i in dense_list:\n",
    "    # Optuna Objective\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 64, 128),\n",
    "            'max_depth': trial.suggest_int('max_depth', 8, 16),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.03, log = True), \n",
    "            'n_estimators': trial.suggest_int('n_estimators', 500, 3000),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 30, 100),\n",
    "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.4, 0.7),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 0.1, 10.0, log = True),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 10.0, log = True)\n",
    "        }\n",
    "\n",
    "        optuna_model = clone(lgbm).set_params(**params)\n",
    "\n",
    "        scores = cross_validate(\n",
    "            estimator = optuna_model, X = x[i], y = y,\n",
    "            scoring = ScoreFarm,\n",
    "            cv = cv,\n",
    "            n_jobs = 4,\n",
    "            verbose = 1\n",
    "        )\n",
    "        \n",
    "        return scores['test_score'].mean()\n",
    "    \n",
    "    # Optimization\n",
    "    os.environ['PYTHONHASHSEED'] = str(42)\n",
    "    study = optuna.create_study(\n",
    "        direction = 'maximize',\n",
    "        sampler = optuna.samplers.TPESampler(seed = 42)\n",
    "    )\n",
    "    study.optimize(objective, n_trials = 30, n_jobs = 3, show_progress_bar = True)\n",
    "\n",
    "    # Result\n",
    "    print(f'\\nBest Parameters (x[\\'{i}\\']):')\n",
    "    print(study.best_params)\n",
    "    print(f'Best CV Score (x[\\'{i}\\']):')\n",
    "    print(study.best_trial.value)\n",
    "    print('\\n========================================\\n')\n",
    "\n",
    "    # Save Parameters & CV Scores\n",
    "    with open(f'params/params_{i}.json', 'w') as f:\n",
    "        json.dump(study.best_trial.params, f, indent = 4)\n",
    "    with open(f'cv_scores/cv_score_{i}.txt', 'w') as f:\n",
    "        f.write(str(study.best_trial.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# LGBM : Model Selection for Sparse Cases\n",
    "# ==================================================\n",
    "\n",
    "# Sparse Cases (n < p)\n",
    "for i in sparse_list:\n",
    "    # Optuna Objective\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 8, 32),\n",
    "            'max_depth': trial.suggest_int('max_depth', 4, 12),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.03, log = True), \n",
    "            'n_estimators': trial.suggest_int('n_estimators', 500, 3000),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 50, 200),\n",
    "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.3, 0.6),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 1.0, 10.0, log = True),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 1.0, 10.0, log = True)\n",
    "        }\n",
    "\n",
    "        optuna_model = clone(lgbm).set_params(**params)\n",
    "\n",
    "        scores = cross_validate(\n",
    "            estimator = optuna_model, X = x[i], y = y,\n",
    "            scoring = ScoreFarm,\n",
    "            cv = cv,\n",
    "            n_jobs = 4,\n",
    "            verbose = 1\n",
    "        )\n",
    "        \n",
    "        return scores['test_score'].mean()\n",
    "    \n",
    "    # Optimization\n",
    "    os.environ['PYTHONHASHSEED'] = str(42)\n",
    "    study = optuna.create_study(\n",
    "        direction = 'maximize',\n",
    "        sampler = optuna.samplers.TPESampler(seed = 42)\n",
    "    )\n",
    "    study.optimize(objective, n_trials = 30, n_jobs = 3, show_progress_bar = True)\n",
    "\n",
    "    # Result\n",
    "    print(f'\\nBest Parameters (x[\\'{i}\\']):')\n",
    "    print(study.best_params)\n",
    "    print(f'Best CV Score (x[\\'{i}\\']):')\n",
    "    print(study.best_trial.value)\n",
    "    print('\\n========================================\\n')\n",
    "\n",
    "    # Save\n",
    "    with open(f'params/params_{i}.json', 'w') as f:\n",
    "        json.dump(study.best_trial.params, f, indent = 4)\n",
    "    with open(f'cv_scores/cv_score_{i}.txt', 'w') as f:\n",
    "        f.write(str(study.best_trial.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# RF (with Top 5 Fingerprints): Preprocessing\n",
    "# ==================================================\n",
    "\n",
    "# Top 5 CV Scores\n",
    "cv_scores = {}\n",
    "for i in md.keys():\n",
    "    with open(f'cv_scores/cv_score_{i}.txt', 'r') as f:\n",
    "        cv_scores[i] = float(f.readline())\n",
    "cv_scores = pd.Series(\n",
    "    list(cv_scores.values()),\n",
    "    index = list(cv_scores.keys())\n",
    ").sort_values(ascending = False)\n",
    "top5_list = list(cv_scores.index[:5])\n",
    "\n",
    "# ==================================================\n",
    "# RF: Model Definition\n",
    "# ==================================================\n",
    "\n",
    "# Base Estimator\n",
    "rf = RandomForestRegressor(\n",
    "    n_jobs = 10,\n",
    "    random_state = 42\n",
    ")\n",
    "\n",
    "# Inputs & Models Dictionary Update\n",
    "additional_list = [i + '_additional' for i in top5_list]\n",
    "for i in additional_list:\n",
    "    i_tmp = i.replace('_additional', '')\n",
    "    x[i] = x[i_tmp].copy()\n",
    "    md[i] = clone(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# RF: Model Selection\n",
    "# ==================================================\n",
    "\n",
    "for i in top5_list:\n",
    "    # Optuna Objective\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 300, 1500, step = 100),\n",
    "            'max_features': trial.suggest_float('max_features', 0.3, 0.6),\n",
    "            'max_samples': trial.suggest_float('max_samples', 0.6, 1.0),\n",
    "        }\n",
    "\n",
    "        optuna_model = clone(rf).set_params(**params)\n",
    "\n",
    "        scores = cross_validate(\n",
    "            estimator = optuna_model, X = x[i], y = y,\n",
    "            scoring = ScoreFarm,\n",
    "            cv = cv,\n",
    "            verbose = 1\n",
    "        )\n",
    "        \n",
    "        return scores['test_score'].mean()\n",
    "    \n",
    "    # Optimization\n",
    "    os.environ['PYTHONHASHSEED'] = str(42)\n",
    "    study = optuna.create_study(\n",
    "        direction = 'maximize',\n",
    "        sampler = optuna.samplers.TPESampler(seed = 42)\n",
    "    )\n",
    "    study.optimize(objective, n_trials = 30, show_progress_bar = True)\n",
    "\n",
    "    # Result\n",
    "    print(f'\\nBest Parameters (x[\\'{i}\\']):')\n",
    "    print(study.best_params)\n",
    "    print(f'Best CV Score (x[\\'{i}\\']):')\n",
    "    print(study.best_trial.value)\n",
    "    print('\\n========================================\\n')\n",
    "\n",
    "    # Save\n",
    "    with open(f'params/params_{i}_additional.json', 'w') as f:\n",
    "        json.dump(study.best_trial.params, f, indent = 4)\n",
    "    with open(f'cv_scores/cv_score_{i}_additional.txt', 'w') as f:\n",
    "        f.write(str(study.best_trial.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "# GROVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# GROVER (https://github.com/tencent-ailab/grover): Preprocessing\n",
    "# ==================================================\n",
    "\n",
    "from grover.model.models import GroverFpGeneration\n",
    "from grover.data.molgraph import MolCollator\n",
    "\n",
    "# `MolCollator`-Compatible Class\n",
    "smiles_list = pd.read_csv('train.csv')['Canonical_Smiles'].to_list()\n",
    "\n",
    "class Record:\n",
    "    __slots__ = (\"smiles\", \"features\", \"targets\")\n",
    "    def __init__(self, s):\n",
    "        self.smiles, self.features, self.targets = s, None, [None]\n",
    "\n",
    "records = [Record(s) for s in smiles_list]\n",
    "\n",
    "collator = MolCollator({}, Namespace(bond_drop_rate = 0, no_cache = True))\n",
    "\n",
    "# Data Loader\n",
    "loader = DataLoader(\n",
    "    records,\n",
    "    batch_size = 128,\n",
    "    shuffle = False,\n",
    "    collate_fn = collator\n",
    ")\n",
    "\n",
    "# ==================================================\n",
    "# GROVER: Model Definition \n",
    "# ==================================================\n",
    "\n",
    "# Pretrained GROVER\n",
    "grover_large = torch.load(\n",
    "    'grover/grover_large.pt', map_location = 'cpu', weights_only = False\n",
    ")\n",
    "grover_state = grover_large['state_dict']\n",
    "\n",
    "# Additional Arguments Required\n",
    "grover_args = grover_large['args']\n",
    "grover_args.cuda = torch.cuda.is_available()\n",
    "grover_args.dropout = 0.1\n",
    "grover_args.fingerprint_source = 'both'\n",
    "\n",
    "# Model Definition\n",
    "grover = GroverFpGeneration(grover_args)\n",
    "grover.load_state_dict(grover_state, strict = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# GROVER + Ridge: Model Selection (Head Only)\n",
    "# ==================================================\n",
    "\n",
    "# Device Setting\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Embedding Extraction\n",
    "grover.to(device).eval()\n",
    "emb_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _, graph_components, *_ in loader:\n",
    "        graph_components = tuple(\n",
    "            t.to(device) if torch.is_tensor(t) else t\n",
    "            for t in graph_components\n",
    "        )\n",
    "        \n",
    "        emb = grover(graph_components, [None]).cpu().numpy()\n",
    "        emb_list.append(emb)\n",
    "        \n",
    "        # Resource Optimization\n",
    "        del emb, graph_components\n",
    "        loader.collate_fn.shared_dict = {}\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "grover_train = pd.DataFrame(np.concatenate(emb_list, axis = 0))\n",
    "\n",
    "# Inputs Dictionary Update\n",
    "x['grover'] = grover_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacement of SGD with Ridge Regression\n",
    "ridge = Ridge(random_state = 42)\n",
    "\n",
    "# Models Dictionary Update\n",
    "md['grover'] = ridge\n",
    "\n",
    "# Regularization Parameter Grid\n",
    "alpha_grid = {\n",
    "    'alpha': np.logspace(1, 3, 40)\n",
    "}\n",
    "\n",
    "# Regularization Parameter Selection\n",
    "grover_cv = GridSearchCV(\n",
    "    estimator = ridge,\n",
    "    param_grid = alpha_grid,\n",
    "    scoring = ScoreFarm,\n",
    "    n_jobs = -1,\n",
    "    cv = cv\n",
    ")\n",
    "\n",
    "grover_cv = grover_cv.fit(x['grover'], y)\n",
    "\n",
    "# Result\n",
    "print('Best Alpha:')\n",
    "print(grover_cv.best_params_)\n",
    "print('CV Score of GROVER:')\n",
    "print(grover_cv.best_score_)\n",
    "\n",
    "# Save\n",
    "with open('params/params_grover.json', 'w') as f:\n",
    "    json.dump(grover_cv.best_params_, f, indent = 4)\n",
    "with open('cv_scores/cv_score_grover.txt', 'w') as f:\n",
    "    f.write(str(grover_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "# Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# Ensemble: Preprocessing\n",
    "# ==================================================\n",
    "\n",
    "# Parameters Setting\n",
    "for i in md.keys():\n",
    "    with open(f'params/params_{i}.json', 'r') as f:\n",
    "        params_tmp = json.load(f)\n",
    "    \n",
    "    md[i] = md[i].set_params(\n",
    "        **params_tmp\n",
    "    )\n",
    "\n",
    "# Meta Features of Each Fold for CV Score\n",
    "ensemble_split_list = []\n",
    "\n",
    "for k, (train_idx, valid_idx) in enumerate(cv.split(train, y)):\n",
    "    y_train = y.iloc[train_idx]\n",
    "    y_valid = y.iloc[valid_idx]\n",
    "\n",
    "    # Untrained Individual Models\n",
    "    md_tmp = {i: clone(j) for i, j in md.items()}\n",
    "    \n",
    "    meta_x_train = {}\n",
    "    meta_x_valid = {}\n",
    "\n",
    "    # Train & Prediction\n",
    "    for i in md_tmp.keys():\n",
    "        x_train = x[i].iloc[train_idx]\n",
    "        x_valid = x[i].iloc[valid_idx]\n",
    "\n",
    "        md_tmp[i] = md_tmp[i].fit(x_train, y_train)\n",
    "\n",
    "        meta_x_train[i] = md_tmp[i].predict(x_train)\n",
    "        meta_x_valid[i] = md_tmp[i].predict(x_valid)\n",
    "\n",
    "    meta_x_train = pd.DataFrame(meta_x_train)\n",
    "    meta_x_valid = pd.DataFrame(meta_x_valid)\n",
    "\n",
    "    # List of Tuples\n",
    "    ensemble_split_list.append((meta_x_train, meta_x_valid, y_train, y_valid))\n",
    "    \n",
    "    print(f'Fold {k + 1} done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# Model Selection: Weighted Average\n",
    "# ==================================================\n",
    "\n",
    "# Optuna Objective\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'base': trial.suggest_int('base', 0, 100),\n",
    "        'rdkit': trial.suggest_int('rdkit', 0, 100),\n",
    "        'atompairs': trial.suggest_int('atompairs', 0, 100),\n",
    "        'topologicaltorsions': trial.suggest_int('topologicaltorsions', 0, 100),\n",
    "        'ecfp4': trial.suggest_int('ecfp4', 0, 100),\n",
    "        'ecfp6': trial.suggest_int('ecfp6', 0, 100),\n",
    "        'maccs': trial.suggest_int('maccs', 0, 100),\n",
    "        'pattern': trial.suggest_int('pattern', 0, 100),\n",
    "        '2dpharmacophore': trial.suggest_int('2dpharmacophore', 0, 100),\n",
    "        'erg': trial.suggest_int('erg', 0, 100),\n",
    "        'mhfp': trial.suggest_int('mhfp', 0, 100),\n",
    "        'secfp': trial.suggest_int('secfp', 0, 100),\n",
    "        'ecfp4_additional': trial.suggest_int('ecfp4_additional', 0, 100),\n",
    "        'ecfp6_additional': trial.suggest_int('ecfp6_additional', 0, 100),\n",
    "        'atompairs_additional': trial.suggest_int('atompairs_additional', 0, 100),\n",
    "        'maccs_additional': trial.suggest_int('maccs_additional', 0, 100),\n",
    "        'topologicaltorsions_additional': trial.suggest_int('topologicaltorsions_additional', 0, 100),\n",
    "        'grover': trial.suggest_int('grover', 0, 100),\n",
    "    }\n",
    "\n",
    "    w = np.array(list(params.values()))\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for _, meta_x_valid, _, y_valid in ensemble_split_list:\n",
    "\n",
    "        y_pred = meta_x_valid.mul(w, axis = 1).sum(axis = 1) / w.sum()\n",
    "        scores.append(ScoreFarm_(y_valid, y_pred))\n",
    "\n",
    "    scores = np.array(scores)\n",
    "    return scores.mean()\n",
    "\n",
    "# Optimization\n",
    "os.environ['PYTHONHASHSEED'] = str(42)\n",
    "study = optuna.create_study(\n",
    "    direction = 'maximize',\n",
    "    sampler = optuna.samplers.TPESampler(seed = 42)\n",
    ")\n",
    "study.optimize(objective, n_trials = 1000, n_jobs = 10, show_progress_bar = True)\n",
    "\n",
    "# Result\n",
    "print('Best Weights:')\n",
    "print(study.best_params)\n",
    "print('CV Score of Weighted Average Ensemble:')\n",
    "print(study.best_trial.value)\n",
    "\n",
    "# Save\n",
    "with open('params/params_ens.json', 'w') as f:\n",
    "    json.dump(study.best_trial.params, f, indent = 4)\n",
    "with open('cv_scores/cv_score_ens.txt', 'w') as f:\n",
    "    f.write(str(study.best_trial.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Whole Trained Models\n",
    "for i in md.keys():\n",
    "    md[i] = md[i].fit(x[i], y)\n",
    "\n",
    "joblib.dump(md, 'model_dict.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
